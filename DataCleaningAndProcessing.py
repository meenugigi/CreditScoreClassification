# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hMg7XZcMyjE0cYYNQkTRWUGRbl57hYUS
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
from sklearn import preprocessing
from flask import Flask, render_template, request, app
import warnings
warnings.filterwarnings('ignore')
pd.set_option('display.float_format', lambda x:'%.5f' % x)
from sklearn.preprocessing import OneHotEncoder,StandardScaler
from sklearn.pipeline import make_pipeline
import pickle

app=Flask(__name__)


# pipe = pickle.load(open("RandomForestClassifierModel.pkl",'rb'))
"""# ***Data Exploration***

---

"""
# def importDataAndExplore():
#     data = pd.read_csv("train.csv")
#     data.head()
#     data.tail()
#     data.shape
#     data.describe()
#     data.nunique()
#     data.isnull().sum()
#     data.info()
#     removing_nulls(data)



"""# ***Data Cleaning & Dealing with NULL values***

"""

le = preprocessing.LabelEncoder()
mapping_payment_behaviour = {}
mapping_credit_mix = {}
mapping_paid_minamount = {}

def removing_nulls():
    data = pd.read_csv("train.csv")
    # Replace all NaN values in Names colum to 'anonymous'
    data['Name'] = data['Name'].fillna('Anonymous')
    # dropping rows with Null values
    data = data[data['Type_of_Loan'].notna()]
    data = data[data['Credit_History_Age'].notna()]
    # iterating through the columns and extracting only the good values
    for column in data[
        ['Num_of_Delayed_Payment', 'Amount_invested_monthly', 'Monthly_Balance', 'Num_of_Loan', 'Outstanding_Debt']]:
        data[column] = data[column].str.extract(r'(\d+)', expand=False)

        # converting column to float and obtaining mean value
        data[column] = data[column].astype(float)

    # Removing the bad values
    s = data[data['Monthly_Balance'] > 100000000]
    data['Monthly_Balance'][data['Monthly_Balance'] > 100000000] = 0

    # Removing the bad values
    data.drop(data.loc[data['Changed_Credit_Limit'] == '_'].index, inplace=True)
    data['Changed_Credit_Limit'] = data['Changed_Credit_Limit'].astype(float)
    return data
    # run method below to retrain model (pkl files)
    # data_cleaning(data)




def data_cleaning(data):
    # removing bad values other than yes and no
    data.loc[data.Payment_of_Min_Amount == "NM", 'Payment_of_Min_Amount'] = "No"
    data.loc[data.Credit_Mix == "_", 'Credit_Mix'] = "Standard"
    data.loc[data.Occupation == "_______", 'Occupation'] = "Others"

    # replacing columns with values < 0 with 0
    for col in data[['Num_Bank_Accounts', 'Delay_from_due_date', 'Num_of_Loan']]:
        data[col][data[col] < 0] = 0

    for col in data[
        ['Amount_invested_monthly', 'Num_of_Delayed_Payment', 'Monthly_Inhand_Salary', 'Num_Credit_Inquiries',
         'Monthly_Balance']]:
        data[col] = data[col].fillna(data[col].mean())

    data['years'] = data['Credit_History_Age'].str.split(' ').str[0]
    data['years'] = data['years'].astype(int)
    data['years'] = data['years'] * 12
    data['months'] = data['Credit_History_Age'].str.split(' ').str[3]
    data['months'] = data['months'].astype(int)
    data['Credit_History_Age'] = data['years'] + data['months']

    # dropped all rows containing bad values
    data.drop(data.index[data['Payment_Behaviour'] == '!@9#%8'], inplace=True)
    return data
    # run method below to retrain model (pkl files)
    # data_encoding(data)


def data_encoding(data):
    data['Occupation'] = le.fit_transform(data.Occupation.values)
    mapping_occupation = dict(zip(le.classes_, range(len(le.classes_))))
    data['Payment_Behaviour'] = le.fit_transform(data.Payment_Behaviour.values)
    mapping_payment_behaviour = dict(zip(le.classes_, range(len(le.classes_))))
    data['Credit_Mix'] = le.fit_transform(data.Credit_Mix.values)
    mapping_credit_mix = dict(zip(le.classes_, range(len(le.classes_))))
    data['Payment_of_Min_Amount'] = le.fit_transform(data.Payment_of_Min_Amount.values)
    mapping_paid_minamount = dict(zip(le.classes_, range(len(le.classes_))))
    return  mapping_occupation,mapping_payment_behaviour, mapping_credit_mix, mapping_paid_minamount
    # run method below to retrain model (pkl files)
    # data_modelling(data)


def reverse_encoding(element, dictionary):
    value = dictionary.get(element)
    print("---", value)
    return value



"""# ***Model Building***

"""
def data_modelling(data):

    # deleting unwanted columns for training model
    data = data.drop(columns=['ID','Month', 'Age', 'SSN', 'Annual_Income', 'Type_of_Loan'])

    # selecting x and y values for dataset
    # x values are independant values.
    # y values are the dependant values
    x = data.iloc[:, [4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]]
    y = data.iloc[:, [21]]

    """## Splitting data into training and testing data"""
    from sklearn.model_selection import train_test_split
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
    naive_bayes(x_train, x_test, y_train, y_test)
    logistic_regression(x_train, x_test, y_train, y_test)
    random_forest(x_train, x_test, y_train, y_test)
    nearest_neighbours(x_train, x_test, y_train, y_test)
    decision_tree(x_train, x_test, y_train, y_test)


    """## *Naive Bayes*"""

def naive_bayes(x_train, x_test, y_train, y_test):
    from sklearn.naive_bayes import GaussianNB
    scaler = StandardScaler()

    nb = GaussianNB()
    pipe_nb = make_pipeline(scaler, nb)
    pipe_nb.fit(x_train, y_train)
    y_pred = pipe_nb.predict(x_test)

    from sklearn.metrics import accuracy_score
    accuracy_score(y_pred, y_test)
    pickle.dump(pipe_nb, open('NaiveBayesModel.pkl', 'wb'))


    """## *Logistic Regression*"""

def logistic_regression(x_train, x_test, y_train, y_test):
    from sklearn.linear_model import LogisticRegression
    scaler = StandardScaler()

    lr = LogisticRegression()
    pipe_lr = make_pipeline(scaler, lr)
    pipe_lr.fit(x_train, y_train)
    y_pred = pipe_lr.predict(x_test)

    from sklearn.metrics import accuracy_score
    accuracy_score(y_pred, y_test)
    pickle.dump(pipe_lr, open('LogisticRegressionModel.pkl', 'wb'))


    """## Random Forest Classifier"""

def random_forest(x_train, x_test, y_train, y_test):
    from sklearn.ensemble import RandomForestClassifier
    scaler = StandardScaler()

    rfc = RandomForestClassifier()
    pipe_rfc = make_pipeline(scaler, rfc)
    pipe_rfc.fit(x_train, y_train)
    y_pred = pipe_rfc.predict(x_test)

    from sklearn.metrics import accuracy_score
    accuracy_score(y_pred, y_test)
    pickle.dump(pipe_rfc,open('RandomForestClassifierModel.pkl','wb'))


    """## *K-Neighbours Classification*"""

def nearest_neighbours(x_train, x_test, y_train, y_test):
    from sklearn.neighbors import KNeighborsClassifier
    scaler = StandardScaler()

    knn = KNeighborsClassifier(n_neighbors=4)
    pipe_knn = make_pipeline(scaler, knn)
    pipe_knn.fit(x_train, y_train)
    y_pred = pipe_knn.predict(x_test)

    from sklearn.metrics import accuracy_score
    accuracy_score(y_pred, y_test)
    pickle.dump(pipe_knn, open('KNearestClassifierModel.pkl', 'wb'))


    """## Decision Tree Classifier"""

def decision_tree(x_train, x_test, y_train, y_test):
    from sklearn.tree import DecisionTreeClassifier
    scaler = StandardScaler()

    dtc = DecisionTreeClassifier()
    pipe_dtc = make_pipeline(scaler, dtc)
    pipe_dtc.fit(x_train, y_train)
    y_pred = pipe_dtc.predict(x_test)

    from sklearn.metrics import accuracy_score
    accuracy_score(y_pred, y_test)
    pickle.dump(pipe_dtc,open('DecisionTreeModel.pkl','wb'))


# run method below to retrain model (pkl files)
if __name__ == '__main__':
  removing_nulls()



